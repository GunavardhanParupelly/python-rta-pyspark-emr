# â­ RTA ETL Pipeline

A scalable and modular ETL pipeline for RTA data, built with **Python** and **PySpark**. This pipeline is designed to handle large-scale data processing, ensuring **data quality**, **fault tolerance**, and seamless integration with distributed systems like **Apache Spark**.

---

## ğŸŒŸ Features

- **Modular Design:** Clean separation of extraction, transformation, and loading (ETL) logic.
- **Scalable:** Built with PySpark to handle large datasets efficiently.
- **Data Validation:** Pre- and post-metrics tracking for data quality assurance.
- **Fault Tolerant:** Logs errors and handles invalid records gracefully.
- **Customizable:** Centralized configuration for Spark, logging, and environment settings.

---

## ğŸ“ Project Structure

```
spark1-master/
â”œâ”€â”€ data/                  # Input and output data files
â”œâ”€â”€ output/                # Pipeline output files
â”œâ”€â”€ src/                   # Source code and configuration
â”‚   â”œâ”€â”€ config/            # Configuration files
â”‚   â”‚   â”œâ”€â”€ logging.config
â”‚   â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”œâ”€â”€ spark_config.py
â”‚   â””â”€â”€ core/              # Core ETL modules
â”‚       â”œâ”€â”€ transformation.py
â”‚       â”œâ”€â”€ extraction.py
â”‚       â”œâ”€â”€ ingestion.py
â”‚       â”œâ”€â”€ persist.py
â”‚       â””â”€â”€ validate.py
â”œâ”€â”€ tests/                 # Unit and integration tests
â”‚   â””â”€â”€ test_transformation.py
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ application.log        # Application log file
â”œâ”€â”€ driver.py              # Main entry point for running the pipeline
â”œâ”€â”€ pytest.ini             # Pytest configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project overview and instructions
```

---

## ğŸ› ï¸ Key Components

1. **Configuration (config)**  
   Centralized configuration for logging, environment variables, and Spark settings.

2. **Core ETL Logic (core)**  
   - `transformation.py`: Data transformation logic.
   - `extraction.py`: Data extraction from source files.
   - `ingestion.py`: Data ingestion into Spark.
   - `persist.py`: Data persistence to output files.
   - `validate.py`: Data validation and metrics tracking.

3. **Tests (tests)**  
   Automated tests for data quality and ETL logic.

4. **Input and Output**  
   - `data`: Input and output datasets.
   - `output`: Processed data files.

---

## ğŸ“Š Metrics Tracking

The pipeline tracks pre-metrics and post-metrics to ensure data quality:

- **Pre-Metrics:** Calculated before data transformation (e.g., row counts, null counts).
- **Post-Metrics:** Calculated after data transformation to validate results.

---

## ğŸ“ Example Workflow

1. **Extract:** Read raw data from data directory.
2. **Transform:** Clean, deduplicate, and validate the data.
3. **Load:** Write the processed data to the output directory.
4. **Validate:** Compare pre- and post-metrics to ensure data quality.

---

## ğŸ“‚ Output Structure

```
output/
â”œâ”€â”€ stage_clean_source/       # Staged clean data
â”œâ”€â”€ gold_fact_registrations/  # Final fact table
â”œâ”€â”€ gold_dim_vehicle/         # Vehicle dimension table
â”œâ”€â”€ gold_dim_manufacturer/    # Manufacturer dimension table
â”œâ”€â”€ gold_dim_rta/             # RTA dimension table
â”œâ”€â”€ error_table/              # Invalid records
```

---

